{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Supermario RL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qssPFD9sOTu3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing necessary libraries\n",
        " %tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import sys, os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import gym\n",
        "from gym.core import Wrapper\n",
        "from gym.spaces.box import Box\n",
        "import cv2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdLZ1XXyzEnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#installing game in colab and importing the gym env of game\n",
        "!pip install gym-super-mario-bros\n",
        "\n",
        "#this is wrapper included by developers of this gym env \n",
        "#\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WScFQ59m0NsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#downloading atari_utils.py \n",
        "!wget -q https://raw.githubusercontent.com/ironman-0-0-7/RL_supermario/master/atari_util.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6KhY2D2Fzix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from gym.core import Wrapper\n",
        "from gym.spaces.box import Box\n",
        "\n",
        "\n",
        "class PreprocessAtari(Wrapper):\n",
        "    def __init__(self, env, height=42, width=42, color=False,\n",
        "                 crop=lambda img: img, n_frames=4, dim_order='pytorch', reward_scale=1):\n",
        "        \"\"\"A gym wrapper that reshapes, crops and scales image into the desired shapes\"\"\"\n",
        "        super(PreprocessAtari, self).__init__(env)\n",
        "        self.img_size = (height, width)\n",
        "        self.crop = crop\n",
        "        self.color = color\n",
        "        self.dim_order = dim_order\n",
        "\n",
        "        self.reward_scale = reward_scale\n",
        "        n_channels = (3 * n_frames) if color else n_frames\n",
        "\n",
        "        obs_shape = {\n",
        "            'theano': (n_channels, height, width),\n",
        "            'pytorch': (n_channels, height, width),\n",
        "            'tensorflow': (height, width, n_channels),\n",
        "        }[dim_order]\n",
        "\n",
        "        self.observation_space = Box(0.0, 1.0, obs_shape)\n",
        "        self.framebuffer = np.zeros(obs_shape, 'float32')\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the game, returns initial frames\"\"\"\n",
        "        self.framebuffer = np.zeros_like(self.framebuffer)\n",
        "        self.update_buffer(self.env.reset())\n",
        "        return self.framebuffer\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Plays the game for 1 step, returns frame buffer\"\"\"\n",
        "        new_img, r, done, info = self.env.step(action)\n",
        "        self.update_buffer(new_img)\n",
        "\n",
        "        return self.framebuffer, r * self.reward_scale, done, info\n",
        "\n",
        "    ### image processing ###\n",
        "\n",
        "    def update_buffer(self, img):\n",
        "        img = self.preproc_image(img)\n",
        "        offset = 3 if self.color else 1\n",
        "        if self.dim_order == 'tensorflow':\n",
        "            axis = -1\n",
        "            cropped_framebuffer = self.framebuffer[:, :, :-offset]\n",
        "        else:\n",
        "            axis = 0\n",
        "            cropped_framebuffer = self.framebuffer[:-offset, :, :]\n",
        "        self.framebuffer = np.concatenate([img, cropped_framebuffer], axis=axis)\n",
        "\n",
        "    def preproc_image(self, img):\n",
        "        \"\"\"what happens to the observation\"\"\"\n",
        "        img = self.crop(img)\n",
        "        img = cv2.resize(img / 255, self.img_size, interpolation=cv2.INTER_LINEAR)\n",
        "        if not self.color:\n",
        "            img = img.mean(-1, keepdims=True)\n",
        "        if self.dim_order != 'tensorflow':\n",
        "            img = img.transpose([2, 0, 1])  # [h, w, c] to [c, h, w]\n",
        "        return img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SM1KzLqJzdo-",
        "colab": {}
      },
      "source": [
        "from atari_util import PreprocessAtari\n",
        "def make_env():\n",
        "    env =  gym_super_mario_bros.make('SuperMarioBros-v0')\n",
        "    env=JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "    env = PreprocessAtari(\n",
        "        env, height=42, width=42,\n",
        "        crop=lambda img: img[60:-30, 5:],\n",
        "        dim_order='tensorflow',\n",
        "        color=False, n_frames=4)\n",
        "    return env\n",
        "\n",
        "\n",
        "\n",
        "env = make_env()\n",
        "obs_shape = env.observation_space.shape\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "print(\"Observation shape:\", obs_shape)\n",
        "print(\"Num actions:\", n_actions)\n",
        "print(\"Action names:\", env.env.env.get_action_meanings())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XozRC2v3p8XK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = env.reset()\n",
        "for _ in range(100):\n",
        "    s, _, _, _ = env.step(env.action_space.sample())\n",
        "\n",
        "plt.title('Game image')\n",
        "plt.imshow(env.render('rgb_array'))\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqnOLSw0p8XU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FsmSCDTp8XZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, Dense, Flatten, Input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz2uJQ30p8Xd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, name, state_shape, n_actions, reuse=False):\n",
        "        \"\"\"A simple actor-critic agent\"\"\"\n",
        "        \n",
        "        with tf.variable_scope(name, reuse=reuse):\n",
        "            \n",
        "            input=Input(shape=state_shape)\n",
        "            x = Conv2D(32, (3, 3), strides=2, activation='relu')(input)\n",
        "            x = Conv2D(32, (3, 3), strides=2, activation='relu')(x)\n",
        "            x = Conv2D(32, (3, 3), strides=2, activation='relu')(x)\n",
        "            x=Flatten()(x)\n",
        "            x=Dense(128, activation='relu')(x)\n",
        "\n",
        "            state_value=Dense(1,activation='linear')(x)\n",
        "            logits=Dense(n_actions,activation='linear')(x)\n",
        "\n",
        "            self.network=Model(inputs=input,outputs=[state_value,logits])\n",
        "\n",
        "            self.state_t = tf.placeholder('float32', [None,] + list(state_shape))\n",
        "            self.agent_outputs = self.symbolic_step(self.state_t)\n",
        "        \n",
        "    def symbolic_step(self, state_t):\n",
        "        \"\"\"Takes agent's previous step and observation, returns next state and whatever it needs to learn (tf tensors)\"\"\"\n",
        "        state_v,logit=self.network(state_t)\n",
        "        logits = logit\n",
        "        state_values =state_v[:,0]\n",
        "        \n",
        "        return logits, state_values\n",
        "    \n",
        "    def step(self, state_t):\n",
        "        \"\"\"Same as symbolic step except it operates on numpy arrays\"\"\"\n",
        "        sess = tf.get_default_session()\n",
        "        return sess.run(self.agent_outputs, {self.state_t: state_t})\n",
        "    \n",
        "    def sample_actions(self, agent_outputs):\n",
        "        \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"\n",
        "        logits, state_values = agent_outputs\n",
        "        policy = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n",
        "        return np.array([np.random.choice(len(p), p=p) for p in policy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_SQGMM8p8Xi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " agent = Agent(\"agent\", obs_shape, n_actions)\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu7ApygEp8Xm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = [env.reset()]\n",
        "logits, value = agent.step(state)\n",
        "print(\"action logits:\\n\", logits)\n",
        "print(\"state values:\\n\", value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5zM5feQp8Xw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EnvBatch:\n",
        "    def __init__(self, n_envs = 10):\n",
        "        \"\"\" Creates n_envs environments and babysits them for ya' \"\"\"\n",
        "        self.envs = [make_env() for _ in range(n_envs)]\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\" Reset all games and return [n_envs, *obs_shape] observations \"\"\"\n",
        "        return np.array([env.reset() for env in self.envs])\n",
        "    \n",
        "    def step(self, actions):\n",
        "        \"\"\"\n",
        "        Send a vector[batch_size] of actions into respective environments\n",
        "        :returns: observations[n_envs, *obs_shape], rewards[n_envs], done[n_envs,], info[n_envs]\n",
        "        \"\"\"\n",
        "        results = [env.step(a) for env, a in zip(self.envs, actions)]\n",
        "        new_obs, rewards, done, infos = map(np.array, zip(*results))\n",
        "        \n",
        "        # reset environments automatically\n",
        "        for i in range(len(self.envs)):\n",
        "            if done[i]:\n",
        "                new_obs[i] = self.envs[i].reset()\n",
        "        \n",
        "        return new_obs, rewards, done, infos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmxerwIMp8X1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_batch = EnvBatch(10)\n",
        "\n",
        "batch_states = env_batch.reset()\n",
        "\n",
        "batch_actions = agent.sample_actions(agent.step(batch_states))\n",
        "\n",
        "batch_next_states, batch_rewards, batch_done, _ = env_batch.step(batch_actions)\n",
        "\n",
        "print(\"State shape:\", batch_states.shape)\n",
        "print(\"Actions:\", batch_actions)\n",
        "print(\"Rewards:\", batch_rewards)\n",
        "print(\"Done:\", batch_done)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMM6T2Vcp8X3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  placeholders \n",
        "states_ph = tf.placeholder('float32', [None,] + list(obs_shape))    \n",
        "next_states_ph = tf.placeholder('float32', [None,] + list(obs_shape))\n",
        "actions_ph = tf.placeholder('int32', (None,))\n",
        "rewards_ph = tf.placeholder('float32', (None,))\n",
        "is_done_ph = tf.placeholder('float32', (None,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-iEgCEpp8X5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# logits[n_envs, n_actions] and state_values[n_envs, n_actions]\n",
        "logits, state_values = agent.symbolic_step(states_ph)\n",
        "next_logits, next_state_values = agent.symbolic_step(next_states_ph)\n",
        "\n",
        "# There is no next state if the episode is done!\n",
        "next_state_values = next_state_values * (1 - is_done_ph)\n",
        "\n",
        "# probabilities and log-probabilities for all actions\n",
        "probs = tf.nn.softmax(logits, axis=-1)            # [n_envs, n_actions]\n",
        "logprobs = tf.nn.log_softmax(logits, axis=-1)     # [n_envs, n_actions]                                                               \n",
        "\n",
        "# log-probabilities only for agent's chosen actions\n",
        "logp_actions = tf.reduce_sum(logprobs * tf.one_hot(actions_ph, n_actions), axis=-1) # [n_envs,]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98dKWXePp8X7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gamma = 0.99\n",
        "advantage = rewards_ph + gamma*next_state_values - state_values\n",
        "entropy =  -tf.reduce_sum(probs * logprobs, 1, name=\"entropy\")\n",
        "target_state_values = rewards_ph+gamma*next_state_values\n",
        "\n",
        "\n",
        "actor_loss = -tf.reduce_mean(logp_actions * tf.stop_gradient(advantage), axis=0) - 0.001 * tf.reduce_mean(entropy, axis=0)\n",
        "critic_loss = tf.reduce_mean((state_values - tf.stop_gradient(target_state_values))**2, axis=0)\n",
        "\n",
        "train_step = tf.train.AdamOptimizer(1e-4).minimize(actor_loss + critic_loss)\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KzuHuKlrNMsy",
        "colab": {}
      },
      "source": [
        "def evaluate(agent, env, n_games=1):\n",
        "    \"\"\"Plays an a game from start till done, returns per-game rewards \"\"\"\n",
        "\n",
        "    game_rewards = []\n",
        "    for _ in range(n_games):\n",
        "        state = env.reset()\n",
        "\n",
        "        total_reward = 0\n",
        "        while True:\n",
        "            action = agent.sample_actions(agent.step([state]))[0]\n",
        "            state, reward, done, info = env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        game_rewards.append(total_reward)\n",
        "    return game_rewards"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWI7Yiu6p8YA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def ewma(x, span=100):\n",
        "    return pd.DataFrame({'x':np.asarray(x)}).x.ewm(span=span).mean().values\n",
        "\n",
        "env_batch = EnvBatch(10)\n",
        "batch_states = env_batch.reset()\n",
        "\n",
        "rewards_history = []\n",
        "entropy_history = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAwsPt7lp8YD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tqdm\n",
        "from IPython.display import clear_output\n",
        "\n",
        "with tqdm.trange(len(entropy_history), 100000) as t:\n",
        "    for i in t:\n",
        "        agent_outputs = agent.step(batch_states)\n",
        "        batch_actions = agent.sample_actions(agent_outputs)\n",
        "        batch_next_states, batch_rewards, batch_done, _ = env_batch.step(batch_actions)\n",
        "\n",
        "       \n",
        "        batch_rewards = batch_rewards * 0.01\n",
        "\n",
        "        feed_dict = {\n",
        "            states_ph: batch_states,\n",
        "            actions_ph: batch_actions,\n",
        "            next_states_ph: batch_next_states,\n",
        "            rewards_ph: batch_rewards,\n",
        "            is_done_ph: batch_done,\n",
        "        }\n",
        "\n",
        "        _, ent_t = sess.run([train_step, entropy], feed_dict)\n",
        "        entropy_history.append(np.mean(ent_t))\n",
        "\n",
        "        batch_states = batch_next_states\n",
        "\n",
        "        if i % 500 == 0:\n",
        "            if i % 2500 == 0:\n",
        "                rewards_history.append(np.mean(evaluate(agent, env, n_games=3)))\n",
        "\n",
        "            clear_output(True)\n",
        "\n",
        "            plt.figure(figsize=[8, 4])\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.plot(rewards_history, label='rewards')\n",
        "            plt.plot(ewma(np.array(rewards_history), span=10), marker='.', label='rewards ewma@10')\n",
        "            plt.title(\"Session rewards\")\n",
        "            plt.grid()\n",
        "            plt.legend()\n",
        "\n",
        "           "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfQ6uHgsp8YF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym.wrappers\n",
        "\n",
        "with gym.wrappers.Monitor(make_env(), directory=\"videos\", force=True) as env_monitor:\n",
        "    final_rewards = evaluate(agent, env_monitor, n_games=3)\n",
        "\n",
        "print(\"Final mean reward:\", np.mean(final_rewards))\n",
        "from pathlib import Path\n",
        "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiQqGlPp1gX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download(video_names[-2]) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}